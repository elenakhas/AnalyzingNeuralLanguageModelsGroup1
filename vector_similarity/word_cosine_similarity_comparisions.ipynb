{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import itertools\n",
    "from probe.load_data import WordInspectionDataset\n",
    "from scipy.spatial.distance import cosine\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\", do_lower_case=True)\n",
    "    feature_extraction_model = BertModel.from_pretrained('bert-large-uncased')\n",
    "    batch_size = 20  # totally arbitrarily chosen\n",
    "\n",
    "    pdata = WordInspectionDataset('vec_sim_test.txt', tokenizer)\n",
    "    dataset = pdata.get_data()\n",
    "    embedding_outputs, encoded_inputs, indices = pdata.bert_word_embeddings(feature_extraction_model,\n",
    "                                                                            pdata.get_encoded(), batch_size)\n",
    "    sentence_embeddings = pdata.aggregate_sentence_embeddings(embedding_outputs, encoded_inputs, indices,\n",
    "                                                              aggregation_metric=torch.mean)\n",
    "\n",
    "    idiom_sentence_indexes = get_idiom_sentences(dataset)\n",
    "\n",
    "    cosine_metrics = [calculate_similarity_metrics(idiom_sent_idx, tokenizer, dataset, embedding_outputs, encoded_inputs) \n",
    "                        for idiom_sent_idx in idiom_sentence_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_metrics(idiom_sent_index, tokenizer, dataset, embedding_outputs, encoded_inputs):\n",
    "    idiom_ex = dataset[idiom_sent_index]\n",
    "    idiom_word_embedding = get_word_embedding(tokenizer, dataset, embedding_outputs, encoded_inputs, idiom_sent_index)\n",
    "    cosine_similarity_metrics = {}\n",
    "\n",
    "    literal_usage_sents = [i for i, ex in enumerate(dataset) if ex.pair_id == idiom_ex.pair_id and \n",
    "                                                            ex.word == idiom_ex.word and not \n",
    "                                                            ex.sentence_id == idiom_ex.sentence_id ]\n",
    "    paraphrase_sents = [i for i, ex in enumerate(dataset) if ex.pair_id == idiom_ex.pair_id \n",
    "                                                            and not ex.word == idiom_ex.word]\n",
    "\n",
    "    literal_usage_embeddings = [get_word_embedding(tokenizer, dataset, embedding_outputs, encoded_inputs, lit_idx) for lit_idx in literal_usage_sents]\n",
    "    paraphrase_embeddings = [get_word_embedding(tokenizer, dataset, embedding_outputs, encoded_inputs, para_idx) for para_idx in paraphrase_sents]\n",
    "\n",
    "    cosine_similarity_metrics['fig_to_literal'] = calculate_cosine_similarity_average([idiom_word_embedding], literal_usage_embeddings)\n",
    "    cosine_similarity_metrics['literal_to_literal'] = calculate_cosine_similarity_average(literal_usage_embeddings)\n",
    "    cosine_similarity_metrics['fig_to_paraphrase'] = calculate_cosine_similarity_average([idiom_word_embedding], paraphrase_embeddings)\n",
    "    cosine_similarity_metrics['literal_to_paraphrase'] = calculate_cosine_similarity_average(literal_usage_embeddings, paraphrase_embeddings)\n",
    "    \n",
    "    return {\n",
    "        'sentence_id': idiom_ex.sentence_id,\n",
    "        'sentence': idiom_ex.sentence,\n",
    "        'word': idiom_ex.word,\n",
    "        'paraphrase_word': dataset[paraphrase_sents[0]].word,\n",
    "        'cosine_similarities': cosine_similarity_metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idiom_sentences(dataset):\n",
    "    return [i for i, ex in enumerate(dataset) if ex.figurative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_for_idiom_sentence(dataset, idiom_sent):\n",
    "    literal_usage_sents = [i for i, ex in enumerate(dataset) if ex.pair_id == idiom_sent.pair_id and \n",
    "                                                                ex.word == idiom_sent.word and not \n",
    "                                                                ex.sentence_id == idiom_sent.sentence_id ]\n",
    "    paraphrase_sents = [i for i, ex in enumerate(dataset) if ex.pair_id == idiom_sent.pair_id and not \n",
    "                                                            ex.word == idiom_sent.word]\n",
    "    return (literal_usage_sents, paraphrase_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(tokenizer, dataset, embedding_outputs, encoded_inputs, dataset_index):\n",
    "    ex = dataset[dataset_index]\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[dataset_index].tolist())\n",
    "    word_index = decoded_tokens.index(ex.word[0])\n",
    "    return embedding_outputs[dataset_index][word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity_average(embeddings_1, embeddings_2=None):\n",
    "    if embeddings_2:\n",
    "        embedding_pairs = list(itertools.product(embeddings_1, embeddings_2))\n",
    "    else:\n",
    "        embedding_pairs = list(itertools.combinations(embeddings_1, 2))\n",
    "\n",
    "    cosine_similarities = [1 - cosine(embedding_1, embedding_2) for embedding_1, embedding_2 in embedding_pairs]\n",
    "    return mean(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\", do_lower_case=True)\n",
    "feature_extraction_model = BertModel.from_pretrained('bert-large-uncased')\n",
    "batch_size = 20  # totally arbitrarily chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af210de292784f8ebca841662879ba07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Feature extraction', max=1, style=ProgressSâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 20/60 sentences, current max sentence length 17\n",
      "processed 40/60 sentences, current max sentence length 20\n",
      "processed 60/60 sentences, current max sentence length 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdata = WordInspectionDataset('vec_sim_test.txt', tokenizer)\n",
    "dataset = pdata.get_data()\n",
    "embedding_outputs, encoded_inputs, indices = pdata.bert_word_embeddings(feature_extraction_model,\n",
    "                                                                            pdata.get_encoded(), batch_size)\n",
    "sentence_embeddings = pdata.aggregate_sentence_embeddings(embedding_outputs, encoded_inputs, indices,\n",
    "                                                              aggregation_metric=torch.mean)\n",
    "\n",
    "idiom_sentence_indexes = get_idiom_sentences(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_metrics = [calculate_similarity_metrics(idiom_sent_idx, tokenizer, dataset, embedding_outputs, encoded_inputs) \n",
    "                        for idiom_sent_idx in idiom_sentence_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence_id': 1,\n",
       "  'sentence': ['the', 'cat', 'is', 'out', 'of', 'the', 'bag', '.'],\n",
       "  'word': ['cat'],\n",
       "  'paraphrase_word': ['secret'],\n",
       "  'cosine_similarities': {'fig_to_literal': 0.8095400002267625,\n",
       "   'literal_to_literal': 0.881085894174046,\n",
       "   'fig_to_paraphrase': 0.5255098730325699,\n",
       "   'literal_to_paraphrase': 0.5296435475349426}},\n",
       " {'sentence_id': 21,\n",
       "  'sentence': ['soon', 'we', \"'\", 're', 'going', 'to', 'hit', 'the', 'sack'],\n",
       "  'word': ['sack'],\n",
       "  'paraphrase_word': ['bed'],\n",
       "  'cosine_similarities': {'fig_to_literal': 0.38966219623883563,\n",
       "   'literal_to_literal': 0.6366828282674154,\n",
       "   'fig_to_paraphrase': 0.3006304681301117,\n",
       "   'literal_to_paraphrase': 0.4710257798433304}},\n",
       " {'sentence_id': 41,\n",
       "  'sentence': ['it', 'is', 'time', 'to', 'bite', 'the', 'bullet'],\n",
       "  'word': ['bullet'],\n",
       "  'paraphrase_word': ['situation'],\n",
       "  'cosine_similarities': {'fig_to_literal': 0.6265564627117581,\n",
       "   'literal_to_literal': 0.8031015197436014,\n",
       "   'fig_to_paraphrase': 0.37428863942623136,\n",
       "   'literal_to_paraphrase': 0.4543931073612637}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
