{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import itertools\n",
    "from probe.load_data import WordInspectionDataset\n",
    "from scipy.spatial.distance import cosine\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\", do_lower_case=True)\n",
    "    feature_extraction_model = BertModel.from_pretrained('bert-large-uncased')\n",
    "    batch_size = 20  # totally arbitrarily chosen\n",
    "\n",
    "    pdata = WordInspectionDataset('vec_sim_test.txt', tokenizer)\n",
    "    dataset = pdata.get_data()\n",
    "    embedding_outputs, encoded_inputs, indices = pdata.bert_word_embeddings(feature_extraction_model,\n",
    "                                                                            pdata.get_encoded(), batch_size)\n",
    "    sentence_embeddings = pdata.aggregate_sentence_embeddings(embedding_outputs, encoded_inputs, indices,\n",
    "                                                              aggregation_metric=torch.mean)\n",
    "\n",
    "    idiom_sentence_indexes = get_idiom_sentences(dataset)\n",
    "\n",
    "    cosine_metrics = [calculate_similarity_metrics(idiom_sent_idx, tokenizer, dataset, embedding_outputs, encoded_inputs) \n",
    "                        for idiom_sent_idx in idiom_sentence_indexes]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_metrics(idiom_sent_index, tokenizer, dataset, embedding_outputs, encoded_inputs):\n",
    "    idiom_ex = dataset[idiom_sent_index]\n",
    "    idiom_word_embedding = get_word_embedding(tokenizer, dataset, embedding_outputs, encoded_inputs, idiom_sent_index)\n",
    "    cosine_similarity_metrics = {}\n",
    "\n",
    "    literal_usage_sents = [i for i, ex in enumerate(dataset) if ex.pair_id == idiom_ex.pair_id and \n",
    "                                                            ex.word == idiom_ex.word and not \n",
    "                                                            ex.sentence_id == idiom_ex.sentence_id ]\n",
    "    paraphrase_sents = [i for i, ex in enumerate(dataset) if ex.pair_id == idiom_ex.pair_id \n",
    "                                                            and not ex.word == idiom_ex.word]\n",
    "\n",
    "    literal_usage_embeddings = [get_word_embedding(tokenizer, dataset, embedding_outputs, encoded_inputs, lit_idx) for lit_idx in literal_usage_sents]\n",
    "    paraphrase_embeddings = [get_word_embedding(tokenizer, dataset, embedding_outputs, encoded_inputs, para_idx) for para_idx in paraphrase_sents]\n",
    "\n",
    "    cosine_similarity_metrics['fig_to_literal'] = calculate_cosine_similarity_average([idiom_word_embedding], literal_usage_embeddings)\n",
    "    cosine_similarity_metrics['literal_to_literal'] = calculate_cosine_similarity_average(literal_usage_embeddings)\n",
    "    cosine_similarity_metrics['fig_to_paraphrase'] = calculate_cosine_similarity_average([idiom_word_embedding], paraphrase_embeddings)\n",
    "    cosine_similarity_metrics['literal_to_paraphrase'] = calculate_cosine_similarity_average(literal_usage_embeddings, paraphrase_embeddings)\n",
    "    \n",
    "    return {\n",
    "        'sentence_id': idiom_ex.sentence_id,\n",
    "        'sentence': idiom_ex.sentence,\n",
    "        'word': idiom_ex.word,\n",
    "        'paraphrase_word': dataset[paraphrase_sents[0]].word,\n",
    "        'cosine_similarities': cosine_similarity_metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idiom_sentences(dataset):\n",
    "    return [i for i, ex in enumerate(dataset) if ex.figurative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_for_idiom_sentence(dataset, idiom_sent):\n",
    "    literal_usage_sents = [i for i, ex in enumerate(dataset) if ex.pair_id == idiom_sent.pair_id and \n",
    "                                                                ex.word == idiom_sent.word and not \n",
    "                                                                ex.sentence_id == idiom_sent.sentence_id ]\n",
    "    paraphrase_sents = [i for i, ex in enumerate(dataset) if ex.pair_id == idiom_sent.pair_id and not \n",
    "                                                            ex.word == idiom_sent.word]\n",
    "    return (literal_usage_sents, paraphrase_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(tokenizer, dataset, embedding_outputs, encoded_inputs, dataset_index):\n",
    "    ex = dataset[dataset_index]\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(encoded_inputs[dataset_index].tolist())\n",
    "    word_index = decoded_tokens.index(ex.word[0])\n",
    "    return embedding_outputs[dataset_index][word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity_average(embeddings_1, embeddings_2=None):\n",
    "    if embeddings_2:\n",
    "        embedding_pairs = list(itertools.product(embeddings_1, embeddings_2))\n",
    "    else:\n",
    "        embedding_pairs = list(itertools.combinations(embeddings_1, 2))\n",
    "\n",
    "    cosine_similarities = [1 - cosine(embedding_1, embedding_2) for embedding_1, embedding_2 in embedding_pairs]\n",
    "    return mean(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\", do_lower_case=True)\n",
    "feature_extraction_model = BertModel.from_pretrained('bert-large-uncased')\n",
    "batch_size = 20  # totally arbitrarily chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1740e3dbd54c44cdbcf418424831aaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Feature extraction', max=1, style=ProgressSâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 20/60 sentences, current max sentence length 17\n",
      "processed 40/60 sentences, current max sentence length 17\n",
      "processed 60/60 sentences, current max sentence length 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdata = WordInspectionDataset('vec_sim_test.txt', tokenizer)\n",
    "dataset = pdata.get_data()\n",
    "embedding_outputs, encoded_inputs, indices = pdata.bert_word_embeddings(feature_extraction_model,\n",
    "                                                                            pdata.get_encoded(), batch_size)\n",
    "sentence_embeddings = pdata.aggregate_sentence_embeddings(embedding_outputs, encoded_inputs, indices,\n",
    "                                                              aggregation_metric=torch.mean)\n",
    "\n",
    "idiom_sentence_indexes = get_idiom_sentences(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_metrics = [calculate_similarity_metrics(idiom_sent_idx, tokenizer, dataset, embedding_outputs, encoded_inputs) \n",
    "                        for idiom_sent_idx in idiom_sentence_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence_id': 1,\n",
       "  'sentence': ['the', 'cat', 'is', 'out', 'of', 'the', 'bag', '.'],\n",
       "  'word': ['cat'],\n",
       "  'paraphrase_word': ['secret'],\n",
       "  'cosine_similarities': {'fig_to_literal': 0.8095400002267625,\n",
       "   'literal_to_literal': 0.881085894174046,\n",
       "   'fig_to_paraphrase': 0.5255098730325699,\n",
       "   'literal_to_paraphrase': 0.5296435475349426}},\n",
       " {'sentence_id': 21,\n",
       "  'sentence': ['soon', 'we', \"'\", 're', 'going', 'to', 'hit', 'the', 'sack'],\n",
       "  'word': ['sack'],\n",
       "  'paraphrase_word': ['bed'],\n",
       "  'cosine_similarities': {'fig_to_literal': 0.3896622094843123,\n",
       "   'literal_to_literal': 0.6366828166776233,\n",
       "   'fig_to_paraphrase': 0.3006304562091827,\n",
       "   'literal_to_paraphrase': 0.47102577686309816}},\n",
       " {'sentence_id': 41,\n",
       "  'sentence': ['it', 'is', 'time', 'to', 'bite', 'the', 'bullet'],\n",
       "  'word': ['bullet'],\n",
       "  'paraphrase_word': ['situation'],\n",
       "  'cosine_similarities': {'fig_to_literal': 0.6265564627117581,\n",
       "   'literal_to_literal': 0.8031015197436014,\n",
       "   'fig_to_paraphrase': 0.37428863942623136,\n",
       "   'literal_to_paraphrase': 0.4543931073612637}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] the cat is out of the bag. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] we saw a cat yesterday. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] the cat is a afraid of dogs. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] there was a cat on the roof. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] how many hours a day will a cat sleep. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] she thought she heard a cat crying. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] nobody wanted to keep the cat. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] the cat had bright blue eyes. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] he asked for a cat for his birthday. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] that is probably just the neighbor's cat. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] everyone knows the secret. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] i want to tell you a secret. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] her guilty secret would soon be revealed. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] she whispered as though imparting a secret [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] you secret is safe with me [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] this secret could not be shared with anyone [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] it wasn't my secret to divulge [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] her past until now has remained a secret [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] there is a secret to their victory [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] mark swore to keep it a secret [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] soon we're going to hit the sack [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] he slung it over one shoulder like a sack of cornmeal [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] they said he had wrapped a garbage sack around his head [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] the man pulled a syringe from a brown paper lunch sack [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] i said, lifting the sack off his shoulder [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] the human body is essentially a meat sack full of goo [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] a sack of rice now costs $ 18 in u. s. dollars [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] frank is walking on the snow with a big sack on his back [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] on the lid of every box was a paper sack [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] but you can be aerobically fit, yet unable to pick up a sack of groceries [SEP] [PAD]\n",
      "\n",
      "[CLS] we will go to bed before long [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] to lie in bed with one's lover is not to be anonymous but to be known [SEP] [PAD]\n",
      "\n",
      "[CLS] stay in bed [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] muddy, also smoking, is sitting on his bed [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] sleep in one bed, and in the morning you'll both be warm and happy as before [SEP]\n",
      "\n",
      "[CLS] she lays it down on the bed between herself and the berry picker [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] i want a job, tv, and a bed with green sheets [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] i would have difficulty going to bed at night without knowing [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] this bed was never meant to be slept on [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] the surface of the bed was firm [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] it is time to bite the bullet [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] these extreme speeds are similar to a fired bullet [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] officers found a bullet hole in the driver's seat [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] lopez was hit in the leg, apparently by the same bullet that struck ortiz [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] he said she was clipped by a bullet during river patrol [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] jones was killed by a bullet through his heart [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] the bullet tore through a major artery and lodged in her spine [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] but the bullet is still in the arm [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] bullet holes were visible on the brick exterior [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] the implant feels like a bullet lodged in his palate [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] the unpleasant situation must be dealt with [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] high w - y ratios that might be considered similar to the current situation [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] this is because the situation is different from one country to another [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] this situation seems optimistic [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] we also considered the common situation of having only the clinical block [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] this was not a fair account of the situation [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] as discussed earlier, this situation will become increasingly dire [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] this is not a situation unique to the nordic heritage museum [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] it is an uneasy situation for the palestinians inside the refugee camps [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "[CLS] the opposite situation was observed for the coastal region of the northeast [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in encoded_inputs:\n",
    "    print(tokenizer.decode(row.tolist()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
