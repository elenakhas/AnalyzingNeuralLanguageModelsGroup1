%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith
\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Instructions for ACL 2020 Proceedings}

\author{Daniel Campos \\
  University of Washington / 4060 East Stevens Way Northeast Seattle, WA 98195 United States \\
  Microsoft / One Microsoft Way Redmond, WA 98052 United States\\
  Affiliation / Address line 3 \\
  \texttt{dacampos@uw.edu} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Our Abstract
\end{abstract}
\section{Introduction}
Understanding words as more than just unique sequences of character codes has always been a core problem for machine learning in natural language processing. Fixed vector representations like Word2Vec \cite{Mikolov2013DistributedRO} and Glove \cite{Pennington2014GloveGV} were the standard solution to this problem for a long time, but have recently been largely replaced by contextualized word embeddings generated by neural language models such as  ELMO \cite{Peters2018DeepCW}, GPT \cite{Radford2018ImprovingLU} and BERT \cite{Devlin2019BERTPO}. Applying these word embeddings has provided state-of-the-art performance on a variety of NLP tasks, and there as been much work recently published that makes an attempt to understand their success. 
\comment{There is a point of view that says including more concrete examples of what other studies did and citing those papers would improve the introduction. There is another point of view that says that says that in the context of this class and even probably most conferences, this entire introduction is unnecessary - no one who doesn't know this stuff is going to be reading our paper. Things to think about.}

Many papers have attempted to investigate how these contextualized word embeddings, as well as earlier non-contextualized embeddings, encode various linguistic properties (see \citet{Belinkov_2019} for a detailed overview). We are interested in following in this vein of analysis to explore whether pre-trained language models are encoding semantic information about idiomatic language in their deeply contextualized embeddings, and whether this information can be used to identify similarity between an idiom and a non-figurative paraphrase.
\comment{@Spacemanidol - I dropped some of your concrete references to other papers here in terms of our proposed methodology. I think these would be very appropriate in our final paper, but we haven't even fixed our methodology yet.}

\citet{bizzoni-lappin-2018-predicting} propose a new corpus for a metaphor paraphrasing task and use it to test a neural network on two classification problems, showing that their model produces results that are significantly correlated with human judgments. In addition to studies investigating how syntactic, semantic, and commonsense \citep{zhou2019evaluating} phenomenon are encoded in contextualized representations, \citet{shwartz2019pain} have presented an examination suite that includes 6 tasks related to lexical composition, and compare results on this suite for 6 different models, concluding that contextualized word representations typically perform better than static word embeddings on such tasks and that all of the models had more success on tasks related to \textit{detecting meaning shift} and more difficulty on \textit{recovering implicit meaning}. 

Our interest in idioms, as multi-word expressions whose meanings cannot simply be determined by the sum of their parts, is clearly related to investigations of lexical composition, and \citet{shwartz2019pain}'s framework that uses classification tasks to test the ability of textual representations to address lexical composition will be a useful model. We propose, however, to explore whether semantic information about idioms is being captured in deep contextualized embeddings by a task more similar to the metaphor paraphrasing classification task presented in \citet{bizzoni-lappin-2018-predicting} and also to compare these findings to an intrinsic evaluation of word similarity similar to that found in \citet{Wang_2019} and \citet{van_Aken_2019}.




\section{Methods}

\subsection{Model}
We intend to build an analysis suite with Hugging Face's Transformer\footnote{https://github.com/huggingface/transformers} and run the same experiments across multiple language models, though these may be limited to BERT and its other incarnations which accept SEP tokens as our Idiom Paraphrase Probing Task requires classification of sentence pairs which. Time allowing, we would also like to train a baseline to compare to, though this is a stretch goal.




\section{Related Work}
\section{Experimental Methodologies}
This section describes the training datasets, intrinsic evaluations,evaluated methods and the Usefulness benchmark.
\section{Evaluation}
\section{Conclusion and Future Work}
\section*{Acknowledgments}

The acknowledgments should go immediately before the references. Do not number the acknowledgments section.
Do not include this section when submitting your paper for review.

\bibliography{anthology,acl2020}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendices}
\label{sec:appendix}

\section{Supplemental Material}
\label{sec:supplemental}
\end{document}
